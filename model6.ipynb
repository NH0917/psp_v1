{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "15FUOCTk29Hg58KMpDbOHuMvPpwVSQ3Lk",
      "authorship_tag": "ABX9TyMp4wRdbJmpNLWamCChFLLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NH0917/psp_v1/blob/main/model6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vaac84M5eTJn",
        "outputId": "82ca025e-49af-494a-9614-d1c2ecde92a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install polars"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pickle\n",
        "import polars as pl\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import pyarrow as pa\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import xgboost as xgb\n"
      ],
      "metadata": {
        "id": "D2uOdnkxeWOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/kaggle/psp/input/train.csv\"\n",
        "label_path = \"/content/drive/MyDrive/kaggle/psp/input/train_labels.csv\"\n",
        "output_path = \"/content/drive/MyDrive/kaggle/psp/model/model6/output\""
      ],
      "metadata": {
        "id": "0d6ONsY1eecw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATS = ['event_name', 'name', 'fqid', 'room_fqid', 'text_fqid']\n",
        "NUMS = ['page', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y',\n",
        "        'hover_duration', 'elapsed_time_diff']\n",
        "\n",
        "name_feature = ['basic', 'undefined', 'close', 'open', 'prev', 'next']\n",
        "event_name_feature = ['cutscene_click', 'person_click', 'navigate_click',\n",
        "       'observation_click', 'notification_click', 'object_click',\n",
        "       'object_hover', 'map_hover', 'map_click', 'checkpoint',\n",
        "       'notebook_click']\n",
        "\n",
        "# from https://www.kaggle.com/code/leehomhuang/catboost-baseline-with-lots-features-inference :\n",
        "fqid_lists = ['worker', 'archivist', 'gramps', 'wells', 'toentry', 'confrontation', 'crane_ranger', 'groupconvo', 'flag_girl', 'tomap', 'tostacks', 'tobasement', 'archivist_glasses', 'boss', 'journals', 'seescratches', 'groupconvo_flag', 'cs', 'teddy', 'expert', 'businesscards', 'ch3start', 'tunic.historicalsociety', 'tofrontdesk', 'savedteddy', 'plaque', 'glasses', 'tunic.drycleaner', 'reader_flag', 'tunic.library', 'tracks', 'tunic.capitol_2', 'trigger_scarf', 'reader', 'directory', 'tunic.capitol_1', 'journals.pic_0.next', 'unlockdoor', 'tunic', 'what_happened', 'tunic.kohlcenter', 'tunic.humanecology', 'colorbook', 'logbook', 'businesscards.card_0.next', 'journals.hub.topics', 'logbook.page.bingo', 'journals.pic_1.next', 'journals_flag', 'reader.paper0.next', 'tracks.hub.deer', 'reader_flag.paper0.next', 'trigger_coffee', 'wellsbadge', 'journals.pic_2.next', 'tomicrofiche', 'journals_flag.pic_0.bingo', 'plaque.face.date', 'notebook', 'tocloset_dirty', 'businesscards.card_bingo.bingo', 'businesscards.card_1.next', 'tunic.wildlife', 'tunic.hub.slip', 'tocage', 'journals.pic_2.bingo', 'tocollectionflag', 'tocollection', 'chap4_finale_c', 'chap2_finale_c', 'lockeddoor', 'journals_flag.hub.topics', 'tunic.capitol_0', 'reader_flag.paper2.bingo', 'photo', 'tunic.flaghouse', 'reader.paper1.next', 'directory.closeup.archivist', 'intro', 'businesscards.card_bingo.next', 'reader.paper2.bingo', 'retirement_letter', 'remove_cup', 'journals_flag.pic_0.next', 'magnify', 'coffee', 'key', 'togrampa', 'reader_flag.paper1.next', 'janitor', 'tohallway', 'chap1_finale', 'report', 'outtolunch', 'journals_flag.hub.topics_old', 'journals_flag.pic_1.next', 'reader.paper2.next', 'chap1_finale_c', 'reader_flag.paper2.next', 'door_block_talk', 'journals_flag.pic_1.bingo', 'journals_flag.pic_2.next', 'journals_flag.pic_2.bingo', 'block_magnify', 'reader.paper0.prev', 'block', 'reader_flag.paper0.prev', 'block_0', 'door_block_clean', 'reader.paper2.prev', 'reader.paper1.prev', 'doorblock', 'tocloset', 'reader_flag.paper2.prev', 'reader_flag.paper1.prev', 'block_tomap2', 'journals_flag.pic_0_old.next', 'journals_flag.pic_1_old.next', 'block_tocollection', 'block_nelson', 'journals_flag.pic_2_old.next', 'block_tomap1', 'block_badge', 'need_glasses', 'block_badge_2', 'fox', 'block_1']\n",
        "text_lists = ['tunic.historicalsociety.cage.confrontation', 'tunic.wildlife.center.crane_ranger.crane', 'tunic.historicalsociety.frontdesk.archivist.newspaper', 'tunic.historicalsociety.entry.groupconvo', 'tunic.wildlife.center.wells.nodeer', 'tunic.historicalsociety.frontdesk.archivist.have_glass', 'tunic.drycleaner.frontdesk.worker.hub', 'tunic.historicalsociety.closet_dirty.gramps.news', 'tunic.humanecology.frontdesk.worker.intro', 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation', 'tunic.historicalsociety.basement.seescratches', 'tunic.historicalsociety.collection.cs', 'tunic.flaghouse.entry.flag_girl.hello', 'tunic.historicalsociety.collection.gramps.found', 'tunic.historicalsociety.basement.ch3start', 'tunic.historicalsociety.entry.groupconvo_flag', 'tunic.library.frontdesk.worker.hello', 'tunic.library.frontdesk.worker.wells', 'tunic.historicalsociety.collection_flag.gramps.flag', 'tunic.historicalsociety.basement.savedteddy', 'tunic.library.frontdesk.worker.nelson', 'tunic.wildlife.center.expert.removed_cup', 'tunic.library.frontdesk.worker.flag', 'tunic.historicalsociety.frontdesk.archivist.hello', 'tunic.historicalsociety.closet.gramps.intro_0_cs_0', 'tunic.historicalsociety.entry.boss.flag', 'tunic.flaghouse.entry.flag_girl.symbol', 'tunic.historicalsociety.closet_dirty.trigger_scarf', 'tunic.drycleaner.frontdesk.worker.done', 'tunic.historicalsociety.closet_dirty.what_happened', 'tunic.wildlife.center.wells.animals', 'tunic.historicalsociety.closet.teddy.intro_0_cs_0', 'tunic.historicalsociety.cage.glasses.afterteddy', 'tunic.historicalsociety.cage.teddy.trapped', 'tunic.historicalsociety.cage.unlockdoor', 'tunic.historicalsociety.stacks.journals.pic_2.bingo', 'tunic.historicalsociety.entry.wells.flag', 'tunic.humanecology.frontdesk.worker.badger', 'tunic.historicalsociety.stacks.journals_flag.pic_0.bingo', 'tunic.historicalsociety.closet.intro', 'tunic.historicalsociety.closet.retirement_letter.hub', 'tunic.historicalsociety.entry.directory.closeup.archivist', 'tunic.historicalsociety.collection.tunic.slip', 'tunic.kohlcenter.halloffame.plaque.face.date', 'tunic.historicalsociety.closet_dirty.trigger_coffee', 'tunic.drycleaner.frontdesk.logbook.page.bingo', 'tunic.library.microfiche.reader.paper2.bingo', 'tunic.kohlcenter.halloffame.togrampa', 'tunic.capitol_2.hall.boss.haveyougotit', 'tunic.wildlife.center.wells.nodeer_recap', 'tunic.historicalsociety.cage.glasses.beforeteddy', 'tunic.historicalsociety.closet_dirty.gramps.helpclean', 'tunic.wildlife.center.expert.recap', 'tunic.historicalsociety.frontdesk.archivist.have_glass_recap', 'tunic.historicalsociety.stacks.journals_flag.pic_1.bingo', 'tunic.historicalsociety.cage.lockeddoor', 'tunic.historicalsociety.stacks.journals_flag.pic_2.bingo', 'tunic.historicalsociety.collection.gramps.lost', 'tunic.historicalsociety.closet.notebook', 'tunic.historicalsociety.frontdesk.magnify', 'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo', 'tunic.wildlife.center.remove_cup', 'tunic.library.frontdesk.wellsbadge.hub', 'tunic.wildlife.center.tracks.hub.deer', 'tunic.historicalsociety.frontdesk.key', 'tunic.library.microfiche.reader_flag.paper2.bingo', 'tunic.flaghouse.entry.colorbook', 'tunic.wildlife.center.coffee', 'tunic.capitol_1.hall.boss.haveyougotit', 'tunic.historicalsociety.basement.janitor', 'tunic.historicalsociety.collection_flag.gramps.recap', 'tunic.wildlife.center.wells.animals2', 'tunic.flaghouse.entry.flag_girl.symbol_recap', 'tunic.historicalsociety.closet_dirty.photo', 'tunic.historicalsociety.stacks.outtolunch', 'tunic.library.frontdesk.worker.wells_recap', 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap', 'tunic.capitol_0.hall.boss.talktogramps', 'tunic.historicalsociety.closet.photo', 'tunic.historicalsociety.collection.tunic', 'tunic.historicalsociety.closet.teddy.intro_0_cs_5', 'tunic.historicalsociety.closet_dirty.gramps.archivist', 'tunic.historicalsociety.closet_dirty.door_block_talk', 'tunic.historicalsociety.entry.boss.flag_recap', 'tunic.historicalsociety.frontdesk.archivist.need_glass_0', 'tunic.historicalsociety.entry.wells.talktogramps', 'tunic.historicalsociety.frontdesk.block_magnify', 'tunic.historicalsociety.frontdesk.archivist.foundtheodora', 'tunic.historicalsociety.closet_dirty.gramps.nothing', 'tunic.historicalsociety.closet_dirty.door_block_clean', 'tunic.capitol_1.hall.boss.writeitup', 'tunic.library.frontdesk.worker.nelson_recap', 'tunic.library.frontdesk.worker.hello_short', 'tunic.historicalsociety.stacks.block', 'tunic.historicalsociety.frontdesk.archivist.need_glass_1', 'tunic.historicalsociety.entry.boss.talktogramps', 'tunic.historicalsociety.frontdesk.archivist.newspaper_recap', 'tunic.historicalsociety.entry.wells.flag_recap', 'tunic.drycleaner.frontdesk.worker.done2', 'tunic.library.frontdesk.worker.flag_recap', 'tunic.humanecology.frontdesk.block_0', 'tunic.library.frontdesk.worker.preflag', 'tunic.historicalsociety.basement.gramps.seeyalater', 'tunic.flaghouse.entry.flag_girl.hello_recap', 'tunic.historicalsociety.closet.doorblock', 'tunic.drycleaner.frontdesk.worker.takealook', 'tunic.historicalsociety.basement.gramps.whatdo', 'tunic.library.frontdesk.worker.droppedbadge', 'tunic.historicalsociety.entry.block_tomap2', 'tunic.library.frontdesk.block_nelson', 'tunic.library.microfiche.block_0', 'tunic.historicalsociety.entry.block_tocollection', 'tunic.historicalsociety.entry.block_tomap1', 'tunic.historicalsociety.collection.gramps.look_0', 'tunic.library.frontdesk.block_badge', 'tunic.historicalsociety.cage.need_glasses', 'tunic.library.frontdesk.block_badge_2', 'tunic.kohlcenter.halloffame.block_0', 'tunic.capitol_0.hall.chap1_finale_c', 'tunic.capitol_1.hall.chap2_finale_c', 'tunic.capitol_2.hall.chap4_finale_c', 'tunic.wildlife.center.fox.concern', 'tunic.drycleaner.frontdesk.block_0', 'tunic.historicalsociety.entry.gramps.hub', 'tunic.humanecology.frontdesk.block_1', 'tunic.drycleaner.frontdesk.block_1']\n",
        "room_lists = ['tunic.historicalsociety.entry', 'tunic.wildlife.center', 'tunic.historicalsociety.cage', 'tunic.library.frontdesk', 'tunic.historicalsociety.frontdesk', 'tunic.historicalsociety.stacks', 'tunic.historicalsociety.closet_dirty', 'tunic.humanecology.frontdesk', 'tunic.historicalsociety.basement', 'tunic.kohlcenter.halloffame', 'tunic.library.microfiche', 'tunic.drycleaner.frontdesk', 'tunic.historicalsociety.collection', 'tunic.historicalsociety.closet', 'tunic.flaghouse.entry', 'tunic.historicalsociety.collection_flag', 'tunic.capitol_1.hall', 'tunic.capitol_0.hall', 'tunic.capitol_2.hall']\n",
        "\n",
        "\n",
        "def feature_enginner_pl(x,grp,use_extra,feature_suffix):\n",
        "\n",
        "    aggs = [\n",
        "        pl.col(\"index\").count().alias(f\"session_number_{feature_suffix}\"),\n",
        "        *[pl.col(c).drop_nulls().n_unique().alias(f\"{c}_unique_{feature_suffix}\") for c in CATS],\n",
        "\n",
        "        *[pl.col(c).quantile(0.1,\"nearest\").alias(f\"{c}_quantile1_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).quantile(0.2,\"nearest\").alias(f\"{c}_quantile2_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).quantile(0.3,\"nearest\").alias(f\"{c}_quantile3_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).quantile(0.4,\"nearest\").alias(f\"{c}_quantile4_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).quantile(0.5,\"nearest\").alias(f\"{c}_quantile5_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).quantile(0.6,\"nearest\").alias(f\"{c}_quantile6_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).quantile(0.7,\"nearest\").alias(f\"{c}_quantile7_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).quantile(0.8,\"nearest\").alias(f\"{c}_quantile8_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).quantile(0.9,\"nearest\").alias(f\"{c}_quantile9_{feature_suffix}\") for c in NUMS],\n",
        "\n",
        "        *[pl.col(c).mean().alias(f\"{c}_mean_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).std().alias(f\"{c}_std_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).min().alias(f\"{c}_min_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).max().alias(f\"{c}_max_{feature_suffix}\") for c in NUMS],\n",
        "\n",
        "\n",
        "        #Event name feature\n",
        "        *[pl.col(\"event_name\").filter(pl.col(\"event_name\") == c).count().alias(f\"{c}_event_name_counts{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in event_name_feature],\n",
        "\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.1,\"nearest\").alias(f\"{c}_ET_quantile1_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.2,\"nearest\").alias(f\"{c}_ET_quantile2_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.3,\"nearest\").alias(f\"{c}_ET_quantile3_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.4,\"nearest\").alias(f\"{c}_ET_quantile4_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.5,\"nearest\").alias(f\"{c}_ET_quantile5_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.6,\"nearest\").alias(f\"{c}_ET_quantile6_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.7,\"nearest\").alias(f\"{c}_ET_quantile7_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.8,\"nearest\").alias(f\"{c}_ET_quantile8_{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.9,\"nearest\").alias(f\"{c}_ET_quantile9_{feature_suffix}\") for c in event_name_feature],\n",
        "\n",
        "        #Name feature\n",
        "        *[pl.col(\"name\").filter(pl.col(\"name\") == c).count().alias(f\"{c}_name_counts{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in name_feature],\n",
        "\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.1,\"nearest\").alias(f\"{c}_ET_quantile1_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.2,\"nearest\").alias(f\"{c}_ET_quantile2_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.3,\"nearest\").alias(f\"{c}_ET_quantile3_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.4,\"nearest\").alias(f\"{c}_ET_quantile4_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.5,\"nearest\").alias(f\"{c}_ET_quantile5_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.6,\"nearest\").alias(f\"{c}_ET_quantile6_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.7,\"nearest\").alias(f\"{c}_ET_quantile7_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.8,\"nearest\").alias(f\"{c}_ET_quantile8_{feature_suffix}\") for c in name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).quantile(0.9,\"nearest\").alias(f\"{c}_ET_quantile9_{feature_suffix}\") for c in name_feature],\n",
        "\n",
        "\n",
        "        #room list feature\n",
        "        *[pl.col(\"room_fqid\").filter(pl.col(\"room_fqid\") == c).count().alias(f\"{c}_room_fqid_counts{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in room_lists],\n",
        "\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.1,\"nearest\").alias(f\"{c}_ET_quantile1_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.2,\"nearest\").alias(f\"{c}_ET_quantile2_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.3,\"nearest\").alias(f\"{c}_ET_quantile3_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.4,\"nearest\").alias(f\"{c}_ET_quantile4_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.5,\"nearest\").alias(f\"{c}_ET_quantile5_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.6,\"nearest\").alias(f\"{c}_ET_quantile6_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.7,\"nearest\").alias(f\"{c}_ET_quantile7_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.8,\"nearest\").alias(f\"{c}_ET_quantile8_{feature_suffix}\") for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).quantile(0.9,\"nearest\").alias(f\"{c}_ET_quantile9_{feature_suffix}\") for c in room_lists],\n",
        "\n",
        "        #fqid list features\n",
        "        *[pl.col(\"fqid\").filter(pl.col(\"fqid\") == c).count().alias(f\"{c}_fqid_counts{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in fqid_lists],\n",
        "\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.1,\"nearest\").alias(f\"{c}_ET_quantile1_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.2,\"nearest\").alias(f\"{c}_ET_quantile2_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.3,\"nearest\").alias(f\"{c}_ET_quantile3_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.4,\"nearest\").alias(f\"{c}_ET_quantile4_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.5,\"nearest\").alias(f\"{c}_ET_quantile5_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.6,\"nearest\").alias(f\"{c}_ET_quantile6_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.7,\"nearest\").alias(f\"{c}_ET_quantile7_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.8,\"nearest\").alias(f\"{c}_ET_quantile8_{feature_suffix}\") for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).quantile(0.9,\"nearest\").alias(f\"{c}_ET_quantile9_{feature_suffix}\") for c in fqid_lists],\n",
        "\n",
        "        #text list features\n",
        "        *[pl.col(\"text_fqid\").filter(pl.col(\"text_fqid\") == c).count().alias(f\"{c}_text_fqid_counts{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in text_lists],\n",
        "\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.1,\"nearest\").alias(f\"{c}_ET_quantile1_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.2,\"nearest\").alias(f\"{c}_ET_quantile2_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.3,\"nearest\").alias(f\"{c}_ET_quantile3_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.4,\"nearest\").alias(f\"{c}_ET_quantile4_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.5,\"nearest\").alias(f\"{c}_ET_quantile5_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.6,\"nearest\").alias(f\"{c}_ET_quantile6_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.7,\"nearest\").alias(f\"{c}_ET_quantile7_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.8,\"nearest\").alias(f\"{c}_ET_quantile8_{feature_suffix}\") for c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).quantile(0.9,\"nearest\").alias(f\"{c}_ET_quantile9_{feature_suffix}\") for c in text_lists],\n",
        "\n",
        "        #location features\n",
        "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\")==c).mean().alias(f\"{c}_ET_mean_x{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\")==c).std().alias(f\"{c}_ET_std_x{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\")==c).max().alias(f\"{c}_ET_max_x{feature_suffix}\") for c in event_name_feature],\n",
        "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\")==c).min().alias(f\"{c}_ET_min_x{feature_suffix}\") for c in event_name_feature],\n",
        "\n",
        "    ]\n",
        "\n",
        "    df = x.groupby([\"session_id\"],maintain_order=True).agg(aggs).sort(\"session_id\")\n",
        "\n",
        "    if use_extra:\n",
        "        if grp=='5-12':\n",
        "            aggs = [\n",
        "                pl.col(\"elapsed_time\").filter((pl.col(\"text\")==\"Here's the log book.\")|(pl.col(\"fqid\")=='logbook.page.bingo')).apply(lambda s: s.max()-s.min()).alias(\"logbook_bingo_duration\"),\n",
        "                pl.col(\"index\").filter((pl.col(\"text\")==\"Here's the log book.\")|(pl.col(\"fqid\")=='logbook.page.bingo')).apply(lambda s: s.max()-s.min()).alias(\"logbook_bingo_indexCount\"),\n",
        "                pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='reader'))|(pl.col(\"fqid\")==\"reader.paper2.bingo\")).apply(lambda s: s.max()-s.min()).alias(\"reader_bingo_duration\"),\n",
        "                pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='reader'))|(pl.col(\"fqid\")==\"reader.paper2.bingo\")).apply(lambda s: s.max()-s.min()).alias(\"reader_bingo_indexCount\"),\n",
        "                pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='journals'))|(pl.col(\"fqid\")==\"journals.pic_2.bingo\")).apply(lambda s: s.max()-s.min()).alias(\"journals_bingo_duration\"),\n",
        "                pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='journals'))|(pl.col(\"fqid\")==\"journals.pic_2.bingo\")).apply(lambda s: s.max()-s.min()).alias(\"journals_bingo_indexCount\"),\n",
        "            ]\n",
        "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
        "            df = df.join(tmp, on=\"session_id\", how='left')\n",
        "\n",
        "        if grp=='13-22':\n",
        "            aggs = [\n",
        "                pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='reader_flag'))|(pl.col(\"fqid\")==\"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(lambda s: s.max()-s.min() if s.len()>0 else 0).alias(\"reader_flag_duration\"),\n",
        "                pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='reader_flag'))|(pl.col(\"fqid\")==\"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(lambda s: s.max()-s.min() if s.len()>0 else 0).alias(\"reader_flag_indexCount\"),\n",
        "                pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='journals_flag'))|(pl.col(\"fqid\")==\"journals_flag.pic_0.bingo\")).apply(lambda s: s.max()-s.min() if s.len()>0 else 0).alias(\"journalsFlag_bingo_duration\"),\n",
        "                pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='journals_flag'))|(pl.col(\"fqid\")==\"journals_flag.pic_0.bingo\")).apply(lambda s: s.max()-s.min() if s.len()>0 else 0).alias(\"journalsFlag_bingo_indexCount\"),\n",
        "            ]\n",
        "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
        "            df = df.join(tmp, on=\"session_id\", how='left')\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "def preprocessing(feature_path,target_path=None):\n",
        "\n",
        "    columns = [\n",
        "    pl.col(\"page\").cast(pl.Float32),\n",
        "    (\n",
        "        (pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1))\n",
        "        .fill_null(0)\n",
        "        .clip(0,1e9)\n",
        "        .over([\"session_id\",\"level_group\"])\n",
        "        .alias(\"elapsed_time_diff\")\n",
        "    ),\n",
        "    (\n",
        "        (pl.col(\"screen_coor_x\") - pl.col(\"screen_coor_x\").shift(1))\n",
        "        .abs()\n",
        "        .over([\"session_id\",\"level_group\"])\n",
        "        .alias(\"location_x_diff\")\n",
        "    ),\n",
        "    (\n",
        "        (pl.col(\"screen_coor_y\") - pl.col(\"screen_coor_y\").shift(1))\n",
        "        .abs()\n",
        "        .over([\"session_id\",\"level_group\"])\n",
        "        .alias(\"location_y_diff\")\n",
        "    ),\n",
        "    pl.col(\"fqid\").fill_null(\"fqid_None\"),\n",
        "    pl.col(\"text_fqid\").fill_null(\"text_fqid_None\")\n",
        "    ]\n",
        "\n",
        "    df = pl.read_csv(feature_path).drop([\"fullscreen\", \"hq\", \"music\"]).with_columns(columns)\n",
        "    df1 = df.filter(pl.col(\"level_group\") == \"0-4\")\n",
        "    df2 = df.filter(pl.col(\"level_group\") == \"5-12\")\n",
        "    df3 = df.filter(pl.col(\"level_group\") == \"13-22\")\n",
        "\n",
        "    del df\n",
        "    gc.collect()\n",
        "\n",
        "    df1 = feature_enginner_pl(df1,grp=\"0-4\",use_extra=True,feature_suffix=\"\")\n",
        "    print('df1 done',df1.shape)\n",
        "    df2 = feature_enginner_pl(df2,grp=\"5-12\",use_extra=True,feature_suffix=\"\")\n",
        "    print('df2 done',df2.shape)\n",
        "    df3 = feature_enginner_pl(df3,grp=\"13-22\",use_extra=True,feature_suffix=\"\")\n",
        "    print('df3 done',df3.shape)\n",
        "\n",
        "\n",
        "    if target_path:\n",
        "        targets = pd.read_csv(target_path)\n",
        "        targets['session'] = targets.session_id.apply(lambda x: int(x.split('_')[0]))\n",
        "        targets['q'] = targets.session_id.apply(lambda x: int(x.split('_')[-1][1:]))\n",
        "\n",
        "    return df1,df2,df3,targets"
      ],
      "metadata": {
        "id": "02yOfGTPnxiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1,df2,df3,targets = preprocessing(train_path,label_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vcoP8S-n5H7",
        "outputId": "c1a5aea0-243a-4bc6-c51d-d676d57d0e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df1 done (11779, 4477)\n",
            "df2 done (11779, 4483)\n",
            "df3 done (11779, 4481)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_params = {\n",
        "    'objective' : 'binary:logistic',\n",
        "    'eval_metric':'logloss',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 4,\n",
        "    'n_estimators': 1000,\n",
        "    #'num_class':2,\n",
        "    #'early_stopping_rounds': 50,\n",
        "    'subsample':0.8,\n",
        "    'colsample_bytree': 0.4,\n",
        "    'tree_method' :'gpu_hist',\n",
        "    'use_label_encoder' : False}"
      ],
      "metadata": {
        "id": "YTp02PZ9oIgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gkf = GroupKFold(n_splits=5)\n",
        "ALL_USERS = df1.session_id.unique()\n",
        "oof = pd.DataFrame(np.zeros((len(ALL_USERS),18)),index=ALL_USERS)\n",
        "FEATURECOL = df1.columns.tolist()\n",
        "FEATURECOL.remove(\"session_id\")\n",
        "models = {}"
      ],
      "metadata": {
        "id": "ItrK9p1CoJUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold(predict_proba,target):\n",
        "\n",
        "  bst_score = 0\n",
        "  bst_threshold = 0\n",
        "\n",
        "  for threshold in np.arange(0.4,1,0.01):\n",
        "    preds = (predict_proba > threshold).astype(\"int\")\n",
        "    m = f1_score(target,preds,average=\"macro\")\n",
        "\n",
        "    if m > bst_score:\n",
        "      bst_score = m\n",
        "      bst_threshold = threshold\n",
        "\n",
        "  return bst_threshold"
      ],
      "metadata": {
        "id": "UtSjvquyoLvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_feature(train):\n",
        "    train[\"year\"] = train[\"session_id\"].apply(lambda x: int(str(x)[:2])).astype(np.uint8)\n",
        "    train[\"month\"] = train[\"session_id\"].apply(lambda x: int(str(x)[2:4])+1).astype(np.uint8)\n",
        "    train[\"day\"] = train[\"session_id\"].apply(lambda x: int(str(x)[4:6])).astype(np.uint8)\n",
        "    train[\"hour\"] = train[\"session_id\"].apply(lambda x: int(str(x)[6:8])).astype(np.uint8)\n",
        "    train[\"minute\"] = train[\"session_id\"].apply(lambda x: int(str(x)[8:10])).astype(np.uint8)\n",
        "    train[\"second\"] = train[\"session_id\"].apply(lambda x: int(str(x)[10:12])).astype(np.uint8)\n",
        "    return train"
      ],
      "metadata": {
        "id": "BxPLRLXUViNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1,df2,df3 = time_feature(df1),time_feature(df2),time_feature(df3)"
      ],
      "metadata": {
        "id": "uRsjyYYHVtzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(1,19):\n",
        "  if t <=3:   df = df1\n",
        "  elif t<=13: df = df2\n",
        "  elif t<=22: df = df3\n",
        "\n",
        "  #train data\n",
        "  train_x = df.set_index(\"session_id\")\n",
        "  train_y = targets.loc[targets.q == t].set_index(\"session\")\n",
        "  dtrain = xgb.DMatrix(train_x,train_y[\"correct\"])\n",
        "\n",
        "  #valid data\n",
        "  valid_x = df.set_index(\"session_id\")\n",
        "  valid_y = targets.loc[targets.q == t].set_index(\"session\")\n",
        "  dvalid = xgb.DMatrix(valid_x,valid_y[\"correct\"])\n",
        "\n",
        "  bst = xgb.train(xgb_params,num_boost_round=200,dtrain=dtrain)\n",
        "\n",
        "  os.makedirs(output_path,exist_ok=True)\n",
        "  model_path = os.path.join(output_path,f\"XGB_question{t}.pickle\")\n",
        "\n",
        "  with open(model_path,\"wb\") as f:\n",
        "    pickle.dump(bst,f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToOwJqMWoN87",
        "outputId": "a9f89ade-01a0-434d-ae28-2ea4827d4091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02:58:06] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:11] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:16] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:20] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:25] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:30] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:35] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:45] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:50] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:55] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:58:59] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:59:04] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:59:09] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:59:14] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:59:20] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:59:25] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "[02:59:30] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"n_estimators\", \"use_label_encoder\" } are not used.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true = oof.copy()\n",
        "for k in range(18):\n",
        "    # GET TRUE LABELS\n",
        "    tmp = targets.loc[targets.q == k+1].set_index('session').loc[ALL_USERS]\n",
        "    true[k] = tmp.correct.values"
      ],
      "metadata": {
        "id": "7zqdbFXTpOOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []; thresholds = []\n",
        "best_score = 0; best_threshold = 0\n",
        "\n",
        "for threshold in np.arange(0,0.81,0.01):\n",
        "    print(f'{threshold:.02f}, ',end='')\n",
        "    preds = (oof.values.reshape((-1))>threshold).astype('int')\n",
        "    m = f1_score(true.values.reshape((-1)), preds, average='macro')\n",
        "    scores.append(m)\n",
        "    thresholds.append(threshold)\n",
        "    if m>best_score:\n",
        "        best_score = m\n",
        "        best_threshold = threshold\n",
        "\n",
        "print(best_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA0B4WZtpPx8",
        "outputId": "4ab41c2a-192a-464b-dbd1-cf7bf7d61f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_threshold)"
      ],
      "metadata": {
        "id": "QOqEUnZCipxW",
        "outputId": "329e5d18-b884-470e-85e1-38fbce6e5fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oof.to_csv(os.path.join(output_path,\"predict_proba.csv\"),index=True)"
      ],
      "metadata": {
        "id": "YSjKvZOOpSE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('When using optimal threshold...')\n",
        "for k in range(18):\n",
        "\n",
        "    # COMPUTE F1 SCORE PER QUESTION\n",
        "    m = f1_score(true[k].values, (oof[k].values>best_threshold).astype('int'), average='macro')\n",
        "    print(f'Q{k}: F1 =',m)\n",
        "\n",
        "# COMPUTE F1 SCORE OVERALL\n",
        "m = f1_score(true.values.reshape((-1)), (oof.values.reshape((-1))>best_threshold).astype('int'), average='macro')\n",
        "print('==> Overall F1 =',m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aewGw2hYpaw",
        "outputId": "b4a2e9ad-dd3a-471d-cfec-fe3a4d382220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When using optimal threshold...\n",
            "Q0: F1 = 0.6651271027310283\n",
            "Q1: F1 = 0.4983824067604884\n",
            "Q2: F1 = 0.5023098949467438\n",
            "Q3: F1 = 0.6665528033987946\n",
            "Q4: F1 = 0.6115122204450527\n",
            "Q5: F1 = 0.6380524840512957\n",
            "Q6: F1 = 0.6311216111130927\n",
            "Q7: F1 = 0.5582838716784975\n",
            "Q8: F1 = 0.6259131073374224\n",
            "Q9: F1 = 0.5622594245880457\n",
            "Q10: F1 = 0.6093060147942555\n",
            "Q11: F1 = 0.5207074424925603\n",
            "Q12: F1 = 0.4688246751373011\n",
            "Q13: F1 = 0.6289100044295481\n",
            "Q14: F1 = 0.583915317084165\n",
            "Q15: F1 = 0.48660806570198006\n",
            "Q16: F1 = 0.5432845776879541\n",
            "Q17: F1 = 0.49680660766894214\n",
            "==> Overall F1 = 0.6912444997308806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('When using optimal threshold...')\n",
        "for k in range(18):\n",
        "\n",
        "    # COMPUTE F1 SCORE PER QUESTION\n",
        "    m = f1_score(true[k].values, (oof[k].values>best_threshold).astype('int'), average='macro')\n",
        "    print(f'Q{k}: F1 =',m)\n",
        "\n",
        "# COMPUTE F1 SCORE OVERALL\n",
        "m = f1_score(true.values.reshape((-1)), (oof.values.reshape((-1))>best_threshold).astype('int'), average='macro')\n",
        "print('==> Overall F1 =',m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eUujhCXpV18",
        "outputId": "20b64dbb-d3ef-4432-e102-5ea38e431ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When using optimal threshold...\n",
            "Q0: F1 = 0.6623777933038076\n",
            "Q1: F1 = 0.4985674586786496\n",
            "Q2: F1 = 0.5100199382385908\n",
            "Q3: F1 = 0.6625190517191553\n",
            "Q4: F1 = 0.6196639024840345\n",
            "Q5: F1 = 0.6332771890537618\n",
            "Q6: F1 = 0.626847174525654\n",
            "Q7: F1 = 0.5606092839143563\n",
            "Q8: F1 = 0.6209902850879223\n",
            "Q9: F1 = 0.5742915911758651\n",
            "Q10: F1 = 0.6149323420363854\n",
            "Q11: F1 = 0.5196250870365735\n",
            "Q12: F1 = 0.47071396834125456\n",
            "Q13: F1 = 0.6287083500507109\n",
            "Q14: F1 = 0.600371065212524\n",
            "Q15: F1 = 0.47606054764599\n",
            "Q16: F1 = 0.5419516909465257\n",
            "Q17: F1 = 0.5001724497581131\n",
            "==> Overall F1 = 0.6917454315292573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_output_path = \"/content/drive/MyDrive/kaggle/psp/model/mdoel1/feature\"\n",
        "\n",
        "df1.to_csv(os.path.join(feature_output_path,\"df1.csv\"),index=False)\n",
        "df2.to_csv(os.path.join(feature_output_path,\"df2.csv\"),index=False)\n",
        "df3.to_csv(os.path.join(feature_output_path,\"df3.csv\"),index=False)"
      ],
      "metadata": {
        "id": "UHbLcCrxt4P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_col(file_name,df,path):\n",
        "  save_path = os.path.join(path,file_name)\n",
        "  f = open(save_path,\"wb\")\n",
        "  pickle.dump(df.columns.tolist(),f)"
      ],
      "metadata": {
        "id": "-qcvLnyNbbj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_col(\"df1_col.pickle\",df1,output_path)\n",
        "save_col(\"df2_col.pickle\",df2,output_path)\n",
        "save_col(\"df3_col.pickle\",df3,output_path)"
      ],
      "metadata": {
        "id": "_Gvrmv65cADR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ubhdud9uc6RD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}